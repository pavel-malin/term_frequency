{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "   \n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname):\n",
    " \n",
    "        vocab = self.surname_vocab\n",
    "        one_hot = np.zeros(len(vocab), dtype=np.float32)\n",
    "        for token in surname:\n",
    "            one_hot[vocab.lookup_token(token)] = 1\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        surname_vector = \\\n",
    "            self._vectorizer.vectorize(row.surname)\n",
    "\n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_surname': surname_vector,\n",
    "                'y_nationality': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    " \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "\n",
    "        intermediate_vector = F.relu(self.fc1(x_in))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "\n",
    "            train_state['early_stopping_step'] += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/4/surname_mlp/vectorizer.json\n",
      "\tmodel_storage/4/surname_mlp/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    surname_csv=\"data/s/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/4/surname_mlp\",\n",
    "    hidden_dim=300,\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "\n",
    "    cuda=False,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fresh!\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "\n",
    "    print(\"Reloading!\")\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "\n",
    "    print(\"Creating fresh!\")\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab), \n",
    "                               hidden_dim=args.hidden_dim, \n",
    "                               output_dim=len(vectorizer.nationality_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malinroot/.local/lib/python3.7/site-packages/ipykernel_launcher.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527d346b68e24446acb3e72b602f0998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malinroot/.local/lib/python3.7/site-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286fc3aa5bdf41caa1d9cab37bc9e791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=120.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malinroot/.local/lib/python3.7/site-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447a3c1bf680474bbf8f3d4951da20df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=val', max=25.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = classifier(batch_dict['x_surname'])\n",
    "\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            y_pred =  classifier(batch_dict['x_surname'])\n",
    "\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.to(\"cpu\").item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred =  classifier(batch_dict['x_surname'])\n",
    "\n",
    "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).view(1, -1)\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
    "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                    prediction['nationality'],\n",
    "                                    prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.nationality_vocab.lookup_index(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_nationality(name, classifier, vectorizer, k=5):\n",
    "    vectorized_name = vectorizer.vectorize(name)\n",
    "    vectorized_name = torch.tensor(vectorized_name).view(1, -1)\n",
    "    prediction_vector = classifier(vectorized_name, apply_softmax=True)\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "    \n",
    "    probability_values = probability_values.detach().numpy()[0]\n",
    "    indices = indices.detach().numpy()[0]\n",
    "    \n",
    "    results = []\n",
    "    for prob_value, index in zip(probability_values, indices):\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "        results.append({'nationality': nationality, \n",
    "                        'probability': prob_value})\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "k = int(input(\"How many of the top predictions to see? \"))\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "    print(\"Sorry! That's more than the # of nationalities we have.. defaulting you to max size :)\")\n",
    "    k = len(vectorizer.nationality_vocab)\n",
    "    \n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "print(\"Top {} predictions:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "    print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                        prediction['nationality'],\n",
    "                                        prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_dataset_csv=\"data/s/surnames.csv\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    output_munged_csv=\"data/s/surnames_with_splits.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = pd.read_csv(args.raw_dataset_csv, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(surnames.nationality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_nationality = collections.defaultdict(list)\n",
    "for _, row in surnames.iterrows():\n",
    "    by_nationality[row.nationality].append(row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "for _, item_list in sorted(by_nationality.items()):\n",
    "    np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_proportion*n)\n",
    "    n_val = int(args.val_proportion*n)\n",
    "    n_test = int(args.test_proportion*n)\n",
    "\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_surnames = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_surnames.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_surnames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_surnames.to_csv(args.output_munged_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "        \n",
    "    def to_serializable(self):\n",
    "\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):\n",
    "\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self._max_surname_length = max_surname_length\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        one_hot_matrix_size = (len(self.surname_vocab), self._max_surname_length)\n",
    "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "                               \n",
    "        for position_index, character in enumerate(surname):\n",
    "            character_index = self.surname_vocab.lookup_token(character)\n",
    "            one_hot_matrix[character_index][position_index] = 1\n",
    "        \n",
    "        return one_hot_matrix\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        max_surname_length = 0\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            max_surname_length = max(max_surname_length, len(row.surname))\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab, \n",
    "                   max_surname_length=contents['max_surname_length'])\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable(), \n",
    "                'max_surname_length': self._max_surname_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # Class weights\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        surname_matrix = \\\n",
    "            self._vectorizer.vectorize(row.surname)\n",
    "\n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_surname': surname_matrix,\n",
    "                'y_nationality': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    " \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    def __init__(self, initial_num_channels, num_classes, num_channels):\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=initial_num_channels, \n",
    "                      out_channels=num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x_surname, apply_softmax=False):\n",
    "        features = self.convnet(x_surname).squeeze(dim=2)\n",
    "       \n",
    "        prediction_vector = self.fc(features)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_train_state(args, model, train_state):\n",
    "\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        else:\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "            train_state['early_stopping_step'] = 0\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_pred_indices = y_pred.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    surname_csv=\"4_1/data/s/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/4/c\",\n",
    "    hidden_dim=100,\n",
    "    num_channels=256,\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    dropout_p=0.1,\n",
    "    cuda=False,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    catch_keyboard_interrupt=True\n",
    ")\n",
    "\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.reload_from_files:\n",
    "\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = SurnameClassifier(initial_num_channels=len(vectorizer.surname_vocab), \n",
    "                               num_classes=len(vectorizer.nationality_vocab),\n",
    "                               num_channels=args.num_channels)\n",
    "\n",
    "classifer = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(weight=dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = classifier(batch_dict['x_surname'])\n",
    "\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            y_pred =  classifier(batch_dict['x_surname'])\n",
    "\n",
    "            loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred =  classifier(batch_dict['x_surname'])\n",
    "\n",
    "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(0)\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
    "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                    prediction['nationality'],\n",
    "                                    prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_nationality(surname, classifier, vectorizer, k=5):\n",
    "    \n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=0)\n",
    "    prediction_vector = classifier(vectorized_surname, apply_softmax=True)\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "\n",
    "    probability_values = probability_values[0].detach().numpy()\n",
    "    indices = indices[0].detach().numpy()\n",
    "    \n",
    "    results = []\n",
    "    for kth_index in range(k):\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(indices[kth_index])\n",
    "        probability_value = probability_values[kth_index]\n",
    "        results.append({'nationality': nationality, \n",
    "                        'probability': probability_value})\n",
    "    return results\n",
    "\n",
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "\n",
    "k = int(input(\"How many of the top predictions to see? \"))\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "    print(\"Sorry! That's more than the # of nationalities we have.. defaulting you to max size :)\")\n",
    "    k = len(vectorizer.nationality_vocab)\n",
    "    \n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "print(\"Top {} predictions:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "    print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                        prediction['nationality'],\n",
    "                                        prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [0, 0, 1, 1]\n",
    "CENTERS = [(-3, -3), (3, 3), (3, -3), (-3, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size=2, output_size=3, \n",
    "                 num_hidden_layers=1, hidden_activation=nn.Sigmoid):\n",
    "\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.module_list = nn.ModuleList()\n",
    "        \n",
    "        interim_input_size = input_size\n",
    "        interim_output_size = hidden_size\n",
    "        \n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.module_list.append(nn.Linear(interim_input_size, interim_output_size))\n",
    "            self.module_list.append(hidden_activation())\n",
    "            interim_input_size = interim_output_size\n",
    "            \n",
    "        self.fc_final = nn.Linear(interim_input_size, output_size)\n",
    "        \n",
    "        self.last_forward_cache = []\n",
    "       \n",
    "    def forward(self, x, apply_softmax=False):\n",
    "\n",
    "        self.last_forward_cache = []\n",
    "        self.last_forward_cache.append(x.to(\"cpu\").numpy())\n",
    "\n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "            self.last_forward_cache.append(x.to(\"cpu\").data.numpy())\n",
    "            \n",
    "        output = self.fc_final(x)\n",
    "        self.last_forward_cache.append(output.to(\"cpu\").data.numpy())\n",
    "\n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_data(batch_size):\n",
    "    assert len(CENTERS) == len(LABELS), 'centers should have equal number labels'\n",
    "    \n",
    "    x_data = []\n",
    "    y_targets = np.zeros(batch_size)\n",
    "    n_centers = len(CENTERS)\n",
    "    \n",
    "    for batch_i in range(batch_size):\n",
    "        center_idx = np.random.randint(0, n_centers)\n",
    "        x_data.append(np.random.normal(loc=CENTERS[center_idx]))\n",
    "        y_targets[batch_i] = LABELS[center_idx]\n",
    "        \n",
    "    return torch.tensor(x_data, dtype=torch.float32), torch.tensor(y_targets, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(perceptron, x_data, y_truth, n_samples=1000, ax=None, epoch=None, \n",
    "                      title='', levels=[0.3, 0.4, 0.5], linestyles=['--', '-', '--']):\n",
    "    _, y_pred = perceptron(x_data, apply_softmax=True).max(dim=1)\n",
    "    y_pred = y_pred.data.numpy()\n",
    "\n",
    "    x_data = x_data.data.numpy()\n",
    "    y_truth = y_truth.data.numpy()\n",
    "\n",
    "\n",
    "    n_classes = len(set(LABELS))\n",
    "\n",
    "    all_x = [[] for _ in range(n_classes)]\n",
    "    all_colors = [[] for _ in range(n_classes)]\n",
    "    \n",
    "    colors = ['black', 'white']\n",
    "    markers = ['o', '*']\n",
    "    \n",
    "    for x_i, y_pred_i, y_true_i in zip(x_data, y_pred, y_truth):\n",
    "        all_x[y_true_i].append(x_i)\n",
    "        if y_pred_i == y_true_i:\n",
    "            all_colors[y_true_i].append(\"white\")\n",
    "        else:\n",
    "            all_colors[y_true_i].append(\"black\")\n",
    "\n",
    "    all_x = [np.stack(x_list) for x_list in all_x]\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "        \n",
    "    for x_list, color_list, marker in zip(all_x, all_colors, markers):\n",
    "        ax.scatter(x_list[:, 0], x_list[:, 1], edgecolor=\"black\", marker=marker, facecolor=color_list, s=100)\n",
    "    \n",
    "        \n",
    "    xlim = (min([x_list[:,0].min() for x_list in all_x]), \n",
    "            max([x_list[:,0].max() for x_list in all_x]))\n",
    "            \n",
    "    ylim = (min([x_list[:,1].min() for x_list in all_x]), \n",
    "            max([x_list[:,1].max() for x_list in all_x]))\n",
    "\n",
    "    \n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        Z = perceptron(torch.tensor(xy, dtype=torch.float32), \n",
    "                       apply_softmax=True)\n",
    "        Z  = Z[:, i].data.numpy().reshape(XX.shape)\n",
    "        ax.contour(XX, YY, Z, colors=colors[i], levels=levels, linestyles=linestyles)\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "    if epoch is not None:\n",
    "        plt.text(xlim[0], ylim[1], \"Epoch = {}\".format(str(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 24\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "x_data, y_truth = get_toy_data(batch_size=1000)\n",
    "\n",
    "x_data = x_data.data.numpy()\n",
    "y_truth = y_truth.data.numpy().astype(np.int64)\n",
    "\n",
    "n_classes = len(set(LABELS))\n",
    "\n",
    "all_x = [[] for _ in range(n_classes)]\n",
    "all_colors = [[] for _ in range(n_classes)]\n",
    "\n",
    "colors = ['black', 'white']\n",
    "markers = ['o', '*']\n",
    "\n",
    "for x_i, y_true_i in zip(x_data, y_truth):\n",
    "    all_x[y_true_i].append(x_i)\n",
    "    all_colors[y_true_i].append(colors[y_true_i])\n",
    "\n",
    "all_x = [np.stack(x_list) for x_list in all_x]\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "\n",
    "for x_list, color_list, marker in zip(all_x, all_colors, markers):\n",
    "    ax.scatter(x_list[:, 0], x_list[:, 1], edgecolor='black', marker=marker, facecolor=\"white\", s=100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.title(\"\");\n",
    "\n",
    "plt.savefig(\"images/data.png\")\n",
    "plt.savefig(\"images/data.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = len(set(LABELS))\n",
    "num_hidden_layers = 0\n",
    "hidden_size = 2 # isn't ever used but we still set it\n",
    "\n",
    "\n",
    "seed = 24\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "mlp1 = MultilayerPerceptron(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_hidden_layers=num_hidden_layers, \n",
    "                            output_size=output_size)\n",
    "print(mlp1)\n",
    "batch_size = 1000\n",
    "\n",
    "x_data_static, y_truth_static = get_toy_data(batch_size)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "visualize_results(mlp1, x_data_static, y_truth_static, \n",
    "                  ax=ax, title='Initial Perceptron State', levels=[0.5])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.savefig('images/perceptron_initial.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "batch_size = 10000\n",
    "n_batches = 10\n",
    "max_epochs = 10\n",
    "\n",
    "loss_change = 1.0\n",
    "last_loss = 10.0\n",
    "change_threshold = 1e-3\n",
    "epoch = 0\n",
    "all_imagefiles = []\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = optim.Adam(params=mlp1.parameters(), lr=lr)\n",
    "cross_ent_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def early_termination(loss_change, change_threshold, epoch, max_epochs):\n",
    "    terminate_for_loss_change = loss_change < change_threshold\n",
    "    terminate_for_epochs = epoch > max_epochs\n",
    "\n",
    "    return terminate_for_epochs\n",
    "\n",
    "while not early_termination(loss_change, change_threshold, epoch, max_epochs):\n",
    "    for _ in range(n_batches):\n",
    "\n",
    "        x_data, y_target = get_toy_data(batch_size)\n",
    "\n",
    "        mlp1.zero_grad()\n",
    "\n",
    "        y_pred = mlp1(x_data).squeeze()\n",
    "\n",
    "        loss = cross_ent_loss(y_pred, y_target.long())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "        loss_change = abs(last_loss - loss_value)\n",
    "        last_loss = loss_value\n",
    "                \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "    visualize_results(mlp1, x_data_static, y_truth_static, ax=ax, epoch=epoch, \n",
    "                      title=f\"{loss_value:0.2f}; {loss_change:0.4f}\")\n",
    "    plt.axis('off')\n",
    "    epoch += 1\n",
    "    all_imagefiles.append(f'images/perceptron_epoch{epoch}_toylearning.png')\n",
    "    plt.savefig(all_imagefiles[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = len(set(LABELS))\n",
    "num_hidden_layers = 1\n",
    "hidden_size = 2\n",
    "\n",
    "seed = 2\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "mlp2 = MultilayerPerceptron(input_size=input_size, \n",
    "                           hidden_size=hidden_size, \n",
    "                           num_hidden_layers=num_hidden_layers, \n",
    "                           output_size=output_size)\n",
    "print(mlp2)\n",
    "batch_size = 1000\n",
    "\n",
    "x_data_static, y_truth_static = get_toy_data(batch_size)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "visualize_results(mlp2, x_data_static, y_truth_static, \n",
    "                  ax=ax, title='Initial 2-Layer MLP State', levels=[0.5])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.savefig('images/mlp2_initial.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "batch_size = 10000\n",
    "n_batches = 10\n",
    "max_epochs = 15\n",
    "\n",
    "loss_change = 1.0\n",
    "last_loss = 10.0\n",
    "change_threshold = 1e-5\n",
    "epoch = 0\n",
    "all_imagefiles = []\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = optim.Adam(params=mlp2.parameters(), lr=lr)\n",
    "cross_ent_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def early_termination(loss_change, change_threshold, epoch, max_epochs):\n",
    "    terminate_for_loss_change = loss_change < change_threshold    \n",
    "    terminate_for_epochs = epoch > max_epochs\n",
    "\n",
    "    return terminate_for_epochs\n",
    "\n",
    "while not early_termination(loss_change, change_threshold, epoch, max_epochs):\n",
    "    for _ in range(n_batches):\n",
    "        x_data, y_target = get_toy_data(batch_size)\n",
    "\n",
    "        mlp2.zero_grad()\n",
    "\n",
    "        y_pred = mlp2(x_data).squeeze()\n",
    "\n",
    "        loss = cross_ent_loss(y_pred, y_target.long())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "        loss_change = abs(last_loss - loss_value)\n",
    "        last_loss = loss_value\n",
    "                \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "    visualize_results(mlp2, x_data_static, y_truth_static, ax=ax, epoch=epoch, \n",
    "                      title=f\"{loss_value:0.2f}; {loss_change:0.4f}\")\n",
    "    plt.axis('off')\n",
    "    epoch += 1\n",
    "    all_imagefiles.append(f'images/mlp2_epoch{epoch}_toylearning.png')\n",
    "    plt.savefig(all_imagefiles[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = len(set(LABELS))\n",
    "num_hidden_layers = 2\n",
    "hidden_size = 2\n",
    "\n",
    "seed = 399\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "mlp3 = MultilayerPerceptron(input_size=input_size, \n",
    "                           hidden_size=hidden_size, \n",
    "                           num_hidden_layers=num_hidden_layers, \n",
    "                           output_size=output_size)\n",
    "print(mlp3)\n",
    "batch_size = 1000\n",
    "\n",
    "x_data_static, y_truth_static = get_toy_data(batch_size)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "visualize_results(mlp3, x_data_static, y_truth_static, \n",
    "                  ax=ax, title='Initial 3-Layer MLP State', levels=[0.5])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.savefig('images/mlp3_initial.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "batch_size = 10000\n",
    "n_batches = 10\n",
    "max_epochs = 10\n",
    "\n",
    "loss_change = 1.0\n",
    "last_loss = 10.0\n",
    "change_threshold = 1e-5\n",
    "epoch = 0\n",
    "all_imagefiles = []\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = optim.Adam(params=mlp3.parameters(), lr=lr)\n",
    "cross_ent_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def early_termination(loss_change, change_threshold, epoch, max_epochs):\n",
    "    terminate_for_loss_change = loss_change < change_threshold    \n",
    "    terminate_for_epochs = epoch > max_epochs\n",
    "\n",
    "    return terminate_for_epochs\n",
    "\n",
    "while not early_termination(loss_change, change_threshold, epoch, max_epochs):\n",
    "    for _ in range(n_batches):\n",
    "        x_data, y_target = get_toy_data(batch_size)\n",
    "\n",
    "        mlp3.zero_grad()\n",
    "\n",
    "        y_pred = mlp3(x_data).squeeze()\n",
    "\n",
    "        loss = cross_ent_loss(y_pred, y_target.long())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "        loss_change = abs(last_loss - loss_value)\n",
    "        last_loss = loss_value\n",
    "                \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "    visualize_results(mlp3, x_data_static, y_truth_static, ax=ax, epoch=epoch, \n",
    "                      title=f\"{loss_value:0.2f}; {loss_change:0.4f}\")\n",
    "    plt.axis('off')\n",
    "    epoch += 1\n",
    "    all_imagefiles.append(f'images/mlp3_epoch{epoch}_toylearning.png')\n",
    "    plt.savefig(all_imagefiles[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "visualize_results(mlp1, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=ax)\n",
    "plt.axis('off');\n",
    "plt.savefig('images/perceptron_final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "visualize_results(mlp2, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=ax)\n",
    "plt.axis('off');\n",
    "plt.savefig('images/mlp2_final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "visualize_results(mlp3, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=ax)\n",
    "plt.axis('off');\n",
    "plt.savefig('images/mlp3_final.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1,2,figsize=(12,5))\n",
    "visualize_results(mlp1, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=axes[0])\n",
    "visualize_results(mlp2, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=axes[1])\n",
    "plt.tight_layout()\n",
    "axes[0].axis('off');\n",
    "axes[1].axis('off');\n",
    "plt.savefig(\"images/perceptron_vs_mlp2.png\")\n",
    "plt.savefig(\"images/figure_4_3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1,2,figsize=(12,5))\n",
    "visualize_results(mlp1, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=axes[0])\n",
    "visualize_results(mlp3, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=axes[1])\n",
    "plt.tight_layout()\n",
    "axes[0].axis('off');\n",
    "axes[1].axis('off');\n",
    "plt.savefig(\"images/perceptron_vs_mlp3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1,3,figsize=(16,5))\n",
    "visualize_results(mlp1, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=axes[0])\n",
    "visualize_results(mlp2, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=axes[1])\n",
    "visualize_results(mlp3, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=axes[2])\n",
    "plt.tight_layout()\n",
    "axes[0].axis('off');\n",
    "axes[1].axis('off');\n",
    "axes[2].axis('off');\n",
    "plt.savefig(\"images/perceptron_vs_mlp2_vs_mlp3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intermediate_representations(mlp_model, plot_title, figsize=(10,2)):\n",
    "    x_data, y_target = get_toy_data(batch_size)\n",
    "\n",
    "    y_pred = mlp_model(x_data, True).detach().numpy()\n",
    "\n",
    "    x_data = x_data.numpy()\n",
    "    y_target = y_target.numpy()\n",
    "\n",
    "    colors = ['black', 'white'] \n",
    "    markers = ['o', '*'] \n",
    "\n",
    "    plot_markers = []\n",
    "    class_zero_indices = []\n",
    "    class_one_indices = []\n",
    "    for i in range(y_target.shape[0]):\n",
    "        if y_target[i] == 0:\n",
    "            class_zero_indices.append(i)\n",
    "        else:\n",
    "            class_one_indices.append(i)\n",
    "    class_zero_indices = np.array(class_zero_indices)\n",
    "    class_one_indices = np.array(class_one_indices)\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(mlp_model.last_forward_cache), figsize=figsize)\n",
    "\n",
    "    for class_index, data_indices in enumerate([class_zero_indices, class_one_indices]):\n",
    "\n",
    "        axes[0].scatter(x_data[data_indices,0], x_data[data_indices,1], edgecolor='black', facecolor=\"white\",\n",
    "                            marker=markers[class_index], s=[200,400][class_index])\n",
    "        axes[0].axis('off')\n",
    "        for i, activations in enumerate(mlp_model.last_forward_cache[1:], 1):\n",
    "            axes[i].scatter(activations[data_indices,0], activations[data_indices,1], edgecolor='black', facecolor=\"white\",\n",
    "                            marker=markers[class_index], s=[200,400][class_index])\n",
    "            axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.suptitle(plot_title, size=15)\n",
    "    plt.subplots_adjust(top=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_representations(mlp1, \n",
    "                                  \"The Perceptron's Input and Intermediate Representation\",\n",
    "                                  figsize=(9, 3))\n",
    "plt.savefig(\"images/perceptron_intermediate.png\")\n",
    "plt.savefig(\"images/figure_4_5.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_representations(mlp2,\n",
    "                                  \"A 2-layer MLP's Input and Intermediate Representation\",\n",
    "                                  figsize=(10, 3))\n",
    "plt.savefig(\"images/mlp2_intermediate.png\")\n",
    "plt.savefig(\"images/figure_4_4.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_representations(mlp3, \n",
    "                                  \"The 3-layer Multilayer Perceptron's Input and Intermediate Representation\",\n",
    "                                  figsize=(13, 3))\n",
    "plt.savefig(\"images/mlp3_intermediate.png\")\n",
    "plt.savefig(\"images/mlp3_intermediate.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
